# -*- coding: utf-8 -*-
"""Proyek Pertama - Model NLP IDCAMP 2023

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19NUXpKfJ4kQaUBM95KUDWmU5jWXjYY7c
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, GlobalMaxPooling1D
import matplotlib.pyplot as plt

lokasi_file = 'Twitter_Data.csv'
df = pd.read_csv(lokasi_file)

df.head(10)

sentimen_mapping = {-1.0: 0, 0: 1, 1: 2}
df['category'] = df['category'].map(sentimen_mapping)

print("Label Sentimen Setelah Pemetaan:")
print(df['category'].unique())

df['category'] = df['category'].astype('category')

df['clean_text'] = df['clean_text'].str.replace(r'@\w+', '')

df.head()

df['clean_text'] = df['clean_text'].astype(str)

df['clean_text'].fillna('', inplace=True)

df.info()

sentiment_counts = df['category'].value_counts()
sentiment_counts

teks = df['clean_text'].values
label = df['category'].values

total_data_teks = len(teks)
total_data_label = len(label)
print("Total Jumlah Data Teks :", total_data_teks)
print("Total Jumlah Data Label :", total_data_label)

teks_latih, teks_test, label_latih, label_test  = train_test_split(teks, label, test_size=0.2)

print("Jumlah Data Pada Set Pelatihan :", len(teks_latih))
print("Jumlah Data Pada Set Validasi :", len(teks_test))

label_encoder = LabelEncoder()
label_latih_encoded = label_encoder.fit_transform(label_latih)
label_test_encoded = label_encoder.transform(label_test)

tokenizer = Tokenizer(num_words=1000000)
tokenizer.fit_on_texts(teks_latih)

sekuens_latih  = tokenizer.texts_to_sequences(teks_latih)
sekuens_test  = tokenizer.texts_to_sequences(teks_test)

panjang_sekuens = max(len(seq) for seq in sekuens_latih )

padded_latih  = pad_sequences(sekuens_latih, maxlen=panjang_sekuens)
padded_test = pad_sequences(sekuens_test , maxlen=panjang_sekuens)

indeks_kata = tokenizer.word_index
ukuran_kata = len(indeks_kata)

checkpoint_filepath = 'Model-NLP-Sentimen-Twitter-IDCAMP-2023.keras'
model_checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_best_only=True,
    monitor='val_accuracy',
    mode='max',
)

input_dim = ukuran_kata + 1

model = Sequential([
    Input(shape=(panjang_sekuens,)),
    Embedding(input_dim, 20),
    LSTM(16, return_sequences=True),
    GlobalMaxPooling1D(),
    Dense(16, activation='relu'),
    Dense(4, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

num_epochs  = 5
history = model.fit(padded_latih, label_latih_encoded , epochs=num_epochs , validation_data=(padded_test, label_test_encoded), callbacks=[model_checkpoint_callback])

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training dan Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training dan Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()